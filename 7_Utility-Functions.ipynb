{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7_Utility_Functions.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XPd9mEe-xQ8w"},"source":["### Importing all required modules"]},{"cell_type":"code","metadata":{"id":"pdI7SdqmL4P5"},"source":["# all imports\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import backend as K\n","from numpy import asarray,zeros,moveaxis\n","import numpy as np\n","import pandas as pd\n","from os import path\n","from tensorflow.keras.initializers import *\n","from tensorflow.keras.models import *\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.callbacks import *\n","from tensorflow.keras.optimizers import *\n","import os,sys,ntpath,fnmatch,shutil,cv2\n","import joblib,time,os.path,itertools\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csc_matrix\n","from sklearn.metrics import *\n","from IPython.display import clear_output\n","from tqdm import tqdm_notebook,tqdm\n","from keras.models import load_model \n","np.random.seed(0)\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","clear_output()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2A-Ulc-8xZKJ"},"source":["### Utility Functions "]},{"cell_type":"code","metadata":{"id":"sZPfTZ1YL4QT"},"source":["def Select_Model(weights_save_path=False):\n","     \n","    \"\"\"\n","    Function to select a Model based on entered choice and load its respective Model weights for Successful Prediction \n","    Input:  weights_save_path <Saved weights path or Boolean>\n","    Returns: Model <Model Object>       \"\"\"\n","\n","    import ntpath,segmentation_models as sm\n","\n","    m=int(input(\"    1.Restnet_50+U-Net    2.Unet    3.DeeplabV3    4.Pspnet    5.Segnet >> Enter a number to Choose a Model:\\n>>> \"))\n","    \n","    if m==1:\n","        \n","        if weights_save_path==False:weights_save_path=\"/content/drive/My Drive/IID_Files1/New_Model_logs_save/Unet_imgnet_resnet50_nlrr.hdf5\"\n","        sent, sm_weight=ntpath.basename(weights_save_path)[12:],\"\"\n","        for ch in sent:\n","            if((ch >= 'a' and ch <= 'z') or (ch >= 'A' and ch <= 'Z')) or (ch >= '0' and ch <= '9'):\n","                sm_weight+=ch\n","            else: break\n","        \n","        Model = sm.Unet(sm_weight,classes=7,input_shape=(224, 480,3),activation='softmax')\n","        Model.load_weights(weights_save_path) if isinstance(weights_save_path, str) else Model.load_weights(\"/content/drive/My Drive/IID_Files1/New_Model_logs_save/Unet_imgnet_resnet50_nlrr.hdf5\")\n","    \n","    elif m==2:\n","        Model = Unet_Segmentation((240, 480,3), 7)\n","        Model.load_weights(weights_save_path) if isinstance(weights_save_path, str) else Model.load_weights(\"/content/drive/My Drive/IID_Files/Models_save/Unet.best.hdf5\")\n","\n","\n","    elif m==3:\n","        Model = Deeplab_V3((240, 480,3), 7) \n","        Model.load_weights(weights_save_path) if isinstance(weights_save_path, str) else Model.load_weights(\"/content/drive/My Drive/IID_Files/Models_save/Deeplab.best.hdf5\") \n","    \n","    elif m==4:\n","        Model=PSPNET((240, 480,3), 7)\n","        Model.load_weights(weights_save_path) if isinstance(weights_save_path, str) else Model.load_weights(\"/content/drive/My Drive/IID_Files/Models_save/Pspnet.best.hdf5\") \n","    \n","    elif m==5:\n","        Model=Segnet_Segmentation((240,480,3), 7, 1)\n","        Model.load_weights(weights_save_path) if isinstance(weights_save_path, str) else Model.load_weights(\"/content/drive/My Drive/IID_Files/Models_save/Segnet.best.hdf5\") \n","\n","    else: raise Exception(\"Wrong Input>>Enter an Integer to choose a Model\")\n","    \n","    return Model\n","\n","def plot_segmentation(images, pred_labels, true_labels=[], plot_limit=5):\n","\n","    \"\"\" \n","    Function to plot Sub-plot of Image, Label and Model Output after prediction is performed\n","    Input  : (images, pred_labels, true_labels) <3dArray>, plot_limit <Int>\n","    Return : None \"\"\" \n","    \n","    if (len(true_labels)==0):\n","        for i in range(images.shape[0]):\n","            if i==plot_limit:return \n","            plt.figure(figsize=(14, 10))\n","            plt.subplot(1, 2, 1)\n","            plt.imshow(cv2.cvtColor(images[i],cv2.COLOR_BGR2RGB))\n","            plt.ylabel('Image',fontsize=16)\n","            plt.subplot(1, 2, 2)\n","            plt.imshow(color_code(pred_labels[i]))\n","            plt.ylabel('Prediction',fontsize=16)\n","            plt.tight_layout()\n","            plt.show()\n","    else:\n","        for i in range(images.shape[0]):\n","            if i==plot_limit:\n","                print(\"  \"+\" ------------------ \"*8)\n","                return \n","            plt.figure(figsize=(28, 20))\n","            plt.subplot(1,3,1)\n","            plt.imshow(cv2.cvtColor(images[i],cv2.COLOR_BGR2RGB))\n","            plt.ylabel('Image',fontsize=28)\n","            plt.subplot(1,3,2)\n","            plt.imshow(color_code(true_labels[i]))\n","            plt.ylabel('Label',fontsize=28)\n","            plt.subplot(1,3,3)\n","            plt.imshow(color_code(pred_labels[i]))\n","            plt.ylabel('Prediction',fontsize=28)\n","            plt.tight_layout()\n","            plt.show()\n","\n","\n","def plot_confusion_matrix(cm, normalize=True, title='Confusion matrix', cmap=plt.cm.Reds):\n","\n","    ''' Function to plot Confusion Matrix for given 2D_Matrix ''' \n","\n","    # ref: https://github.com/scikit-learn/scikit-learn/issues/12700\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    classes_dict={'Drivable': 0, 'Non Drivable': 1, 'Living Things': 2, 'Vehicles': 3, 'Road Side Objects': 4, 'Far Objects': 5, 'Sky': 6}\n","    classes=list(list(classes_dict.keys()))\n","    plt.figure(figsize=(8,8))\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title,fontsize=16)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=90,fontsize=12)\n","    plt.yticks(tick_marks, classes,fontsize=12)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label',fontsize=18)\n","    plt.xlabel('Predicted label',fontsize=18)\n","    plt.show()\n","\n","\n","def Print_result(Mean_MIoU, cf_matrix, Accuracy, true_labels_org, pred_labels_org, cr=True):\n","\n","    ''' Function to print all Final computed Results '''\n","\n","    # printing results\n","    print(\"\\nPrinting Results:>>\\n\")\n","    print('-----------------------')\n","    print('|      MIOU Score     |')\n","    print('-----------------------')\n","    print('\\n   MIOU Score: {}\\n'.format(np.round(np.mean(Mean_MIoU),4)))\n","\n","    print('-----------------------')\n","    print('|   Accuracy Score   |')\n","    print('-----------------------')\n","    print('\\n   Accuracy Score: {}\\n'.format(np.round(np.mean(Accuracy),4)))\n","\n","    print('-----------------------')\n","    print('|  Confusion Matrix   |')\n","    print('-----------------------')\n","    plot_confusion_matrix(cf_matrix,normalize=True)\n","    \n","    if cr==True:\n","        print('-------------------------')\n","        print('| Classifiction Report  |')\n","        print('-------------------------')\n","        print(\"\\n\",classification_report(true_labels_org.ravel(),pred_labels_org.ravel()))\n","        print({'Drivable': 0, 'Non Drivable': 1, 'Living Things': 2, 'Vehicles': 3, 'Road Side Objects': 4, 'Far Objects': 5, 'Sky': 6})\n","    \n","    return \n","\n","def Load_For_Prediction(String):\n","    \n","    \"\"\"\n","    Function to Load Serialized preprocessed Data of Train, Val and Test based on choice for Prediction\n","    Input  : String <String>\n","    Return : prep_train_img_files_save <X>, prep_train_label_files_save <Y> \"\"\"\n","\n","    global prep_train_img_files_save, prep_train_label_files_save\n","\n","    if String ==\"Train_data\":\n","        prep_train_img_files_save=joblib.load(\"/content/drive/My Drive/IID_Files/Data_Save_240*480/prep_train_img_files_save_part1\")\n","        prep_train_label_files_save=joblib.load(\"/content/drive/My Drive/IID_Files/Data_Save_240*480/prep_train_label_files_save_part1\")    \n","    elif String ==\"Val_data\":\n","        prep_train_img_files_save=joblib.load(\"/content/drive/My Drive/IID_Files/Data_Save_240*480/prep_val_img_files_save\")\n","        prep_train_label_files_save=joblib.load(\"/content/drive/My Drive/IID_Files/Data_Save_240*480/prep_val_label_files_save1\")\n","    elif String ==\"Test_data\":\n","        prep_train_img_files_save=joblib.load(\"/content/drive/My Drive/IID_Files/Data_Save_240*480/prep_train_img_files_save_part2\")\n","        prep_train_label_files_save=joblib.load(\"/content/drive/My Drive/IID_Files/Data_Save_240*480/prep_train_label_files_save_part2\")\n","    else:\n","        raise Exception(\"Enter one of the string\\n'Train_data' or 'Val_data' or 'Test_data'\")\n","    \n","    return prep_train_img_files_save, prep_train_label_files_save\n","\n","def plot_training_result(out):\n","\n","    ''' Function to plot Epoch vs Crossentropy and Epoch vs MIOU graph after Traning of a Model ''' \n","\n","    # Sub_plot with two figures\n","    figure, loc_ind = plt.subplots(1, 2,figsize=(15,5))\n","\n","    # Obtain values to plot \n","    miou, val_miou= out.history['miou'], out.history['val_miou']\n","    loss, val_loss= out.history['loss'], out.history['val_loss']\n","\n","    # epoch list\n","    num_epochs = list(range(1,len(miou)+1))\n","\n","    # ploting Epoch vs Crossentropy Loss    \n","    loc_ind[0].plot(num_epochs,loss,'r',label='train loss',linewidth=1.25)\n","    loc_ind[0].plot(num_epochs,val_loss,'g',label='validation loss',linewidth=1.25)\n","    loc_ind[0].set_ylabel('Categorical Crossentropy Loss',fontsize=14)\n","    loc_ind[0].set_xlabel('Epoch',fontsize=14)\n","    loc_ind[0].set_title('Epoch vs Crossentropy Loss',fontsize=16)\n","    loc_ind[0].grid()\n","    loc_ind[0].legend()\n","\n","    # Epoch vs Mean Intersection over union\n","    loc_ind[1].plot(num_epochs,miou,linestyle='--',marker='o',color=\"deeppink\",label='train miou',linewidth=1.25)\n","    loc_ind[1].plot(num_epochs,val_miou,linestyle='--',marker='o',color=\"dodgerblue\",label='val miou',linewidth=1.25)\n","    loc_ind[1].set_ylabel('Mean Intersection over union',fontsize=14)\n","    loc_ind[1].set_xlabel('Epoch',fontsize=14)\n","    loc_ind[1].set_title('Epoch vs Mean Intersection over union',fontsize=16)\n","    loc_ind[1].grid()\n","    loc_ind[1].legend()\n","    plt.tight_layout(pad=3.0)\n","    plt.show() \n","\n","def color_code(le_pred):\n","\n","    \"\"\" \n","    Function to Color Code given Label Mask using RGB-Color to make it suitable to Display  \n","    Input  : le_pred <2D_Array>\n","    Return : col_pred <3d_Array>   \"\"\"\n","    \n","    col_pred = moveaxis(np.repeat(le_pred[:, :, np.newaxis], 3, axis=2), -1, 0)    \n","    color_code=[[128, 64, 128], [72, 98, 91], [255, 204, 54], [220, 20, 60], [147, 114, 178], [132, 91, 83], [70, 70, 70], [105, 143, 35]]         \n","    m, n= col_pred.shape[1], col_pred.shape[2]\n","    for k in range(3):\n","        for i in range(m):\n","            for j in range(n):\n","                index=7 if int(col_pred[k][i][j])==255 else int(col_pred[k][i][j])\n","                col_pred[k][i][j]=color_code[index][k]\n","    col_pred=np.moveaxis(col_pred, 0, -1)\n","    return col_pred\n","\n","\n","def image_prepare(data,batch_files,Model):\n","    \n","    '''Read and preprocess images to generete batch of images for prediction'''\n","    \n","    height,width,n_classes,image=Model.input_shape[1],480,7,[]\n","    for img_i in range(len(batch_files)):\n","        img = cv2.imread(data+batch_files[img_i])\n","        img = cv2.resize(img,(width,height))\n","        img = np.float32(img)/255\n","        image.append(img)\n","    image=np.array(image)\n","    return image\n","\n","\n","def image_prepare_j(slice_saved,Model):\n","    '''Function to get samples batch wise from saved data for Prediction'''\n","    return np.array(slice_saved)\n","\n","\n","def label_prepare_j(slice_saved,Model):\n","    \n","    '''Function to get Mask batch wise from saved data for Prediction'''\n","    \n","    height,width,n_classes=Model.input_shape[1], 480, 7\n","    ar=np.empty((len(slice_saved),n_classes,height,width), dtype=np.uint8)\n","    for j in range(len(slice_saved)):\n","        for i in range(n_classes):\n","            ar[j][i]=slice_saved[j][i].todense()\n","    ar = moveaxis(ar, 1, 3)\n","    return ar\n","\n","\n","def label_prepare(data,batch_files,Model):\n","    \n","    '''Read and preprocess Label Mask to generete batch of Labels for prediction'''\n","    \n","    height,width,n_classes,labels=Model.input_shape[1],480,7,[]\n","    for i in range(len(batch_files)):\n","        label = np.zeros((height, width, n_classes)).astype(np.uint8)\n","        img = cv2.imread(data+batch_files[i])\n","        img = cv2.resize(img,(width,height))\n","        img1 = img[:,:,0]\n","        for i in range(n_classes):\n","            label[:,:,i] = (img1==i).astype(np.uint8)\n","        labels.append(label)\n","    labels=np.array(labels)\n","    return labels\n","\n","\n","def prob_to_label(cf_matrix, Accuracy, predictions, label=False):\n","    \n","    ''' Function to get Actual Labels from Predicted probabilities to compute confusion matrix and Accuracy '''\n","\n","    pred_Out,true_Out,pred_labels,true_labels=[],[],[],[]\n","    predictions = moveaxis(predictions[np.newaxis,:, :,:] if (len(predictions.shape)==3) else predictions, 3, 1)\n","    if isinstance(label,np.ndarray):label = moveaxis(label[np.newaxis,:, :,:] if (len(label.shape)==3) else label, 3, 1)\n","    \n","    for p_index in range(predictions.shape[0]):\n","\n","        p1 = np.where(predictions[p_index]<0.5, predictions[p_index], 1)# if a[ijk]>=0.5 then a[ijk]=1 (False)\n","        p1 = np.where(p1==1, p1, 0).astype(int)\n","        w = np.argmax(p1, axis=0).astype(int) # 248*480 matrix with 1-6\n","        \n","        if isinstance(label,np.ndarray):\n","\n","            p2= np.where(label[p_index]==1, label[p_index], 0)\n","            v = np.argmax(label[p_index], axis=0).astype(int)   # 240*480  \n","            Accuracy.append(np.round(accuracy_score(v.ravel(), w.ravel()),4))\n","\n","            cf_matrix=np.add(cf_matrix, confusion_matrix(v.ravel(), w.ravel(),labels=[0,1,2,3,4,5,6]))\n","            true_labels.append(v),true_Out.append(p2)          \n","        \n","        pred_labels.append(w),pred_Out.append(p1)\n","    \n","    if isinstance(label,np.ndarray):true_Out=moveaxis(np.array(true_Out), 1, 3)\n","    pred_Out=moveaxis(np.array(pred_Out), 1, 3)\n","    \n","    return pred_Out, pred_labels, true_Out, true_labels, cf_matrix, Accuracy\n","\n","\n","def Intersect_over_union(y_val, y_pred, Mean_MIoU):\n","    \n","    \"\"\"\n","    Function to compute Intersect_Over_Union for given set of Samples Inputs \n","    Input  : y_val <4D_Array>, y_pred <4D_Array>, Mean_MIoU <List>\n","    Return : Mean_MIoU <List>      \"\"\" \n","\n","    for index in range(y_pred.shape[0]):\n","\n","        class_iou ,n_classes=[],7\n","        y_predi = np.argmax(y_pred[index], axis=2) \n","        y_truei = np.argmax(y_val[index], axis=2)\n","        \n","        for c in range(n_classes):\n","            TP = np.sum((y_truei == c) & (y_predi == c))\n","            FP = np.sum((y_truei != c) & (y_predi == c))\n","            FN = np.sum((y_truei == c) & (y_predi != c)) \n","            IoU = TP / (TP + FP + FN) if (TP + FP + FN)>0 else 0\n","            class_iou.append(IoU)\n","        \n","        MIoU=sum(class_iou)/n_classes\n","        Mean_MIoU.append(MIoU)\n","    \n","    return Mean_MIoU"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BjqPc1giygoj"},"source":["### Unet Definition for prediction"]},{"cell_type":"code","metadata":{"id":"2pR6Q9JmL4Qc"},"source":["def Unet_Segmentation(input_shape, n_classes):      \n","    \n","    def Unet_En_Blocks(Block_Number, Name, Filters, Kernel_Size, Pool_size, Previous_layer, initialize=\"he_normal\"):\n","        \n","        MaxPool = tf.keras.layers.MaxPooling2D(pool_size=Pool_size, name= Name+\"_Maxpool\")(Previous_layer) if Block_Number>1 else Previous_layer\n","        Convolution1 = tf.keras.layers.Conv2D(Filters, Kernel_Size, name= Name+\"_Conv1\", activation = 'relu', kernel_initializer= initialize, padding='same')(MaxPool)\n","        Convolution2 = tf.keras.layers.Conv2D(Filters, Kernel_Size, name= Name+\"_Conv2\", activation = 'relu', kernel_initializer= initialize, padding='same')(Convolution1)\n","        \n","        return Convolution2\n"," \n","    def Unet_Dec_Blocks(Block_Number, Name, Filters, Kernel_Size, Previous_layer, Layer_to_Concatenate, initialize=\"he_normal\"):\n","        \n","        Up_Sample = tf.keras.layers.UpSampling2D(size=(2, 2), name= Name+\"_Upsample\")(Previous_layer)\n","        Up_Convolution = tf.keras.layers.Conv2D(Filters,(2,2), name= Name+\"_UpConv\", activation = 'relu', padding = 'same',kernel_initializer=initialize)(Up_Sample)\n","        \n","        Concatenated_Layer=tf.keras.layers.Concatenate(axis=3, name= Name+\"_Concat\")([Layer_to_Concatenate,Up_Convolution])\n","                \n","        Convolution1 = tf.keras.layers.Conv2D(Filters, Kernel_Size, name= Name+\"_Conv1\", activation = 'relu', kernel_initializer= initialize, padding ='same')(Concatenated_Layer)\n","        Convolution2 = tf.keras.layers.Conv2D(Filters, Kernel_Size, name= Name+\"_Conv2\", activation = 'relu', kernel_initializer= initialize, padding ='same')(Convolution1)\n","        \n","        if Block_Number==4:\n","            \n","            Convolution3 = tf.keras.layers.Conv2D(n_classes, Kernel_Size, name= \"Final_Conv\", activation = 'relu', kernel_initializer= initialize, padding ='same')(Convolution2)\n","            Output=Activation('softmax', name=\"Softmax\")(Convolution3)\n","            \n","            return Output\n","        \n","        return Convolution2\n"," \n","    Input_layer = tf.keras.layers.Input(shape=input_shape)\n","    En_Block1 = Unet_En_Blocks(1, \"En_Block1\", 64,  (3,3), (2,2), Input_layer)\n","    En_Block2 = Unet_En_Blocks(2, \"En_Block2\", 128, (3,3), (2,2), En_Block1)\n","    En_Block3 = Unet_En_Blocks(3, \"En_Block3\", 256, (3,3), (2,2), En_Block2)\n","    En_Block4 = Unet_En_Blocks(4, \"En_Block4\", 512, (3,3), (2,2), En_Block3)\n","    En_Block5 = Unet_En_Blocks(5, \"En_Block5\", 1024, (3,3), (2,2), En_Block4)\n"," \n","    Dec_Block1 = Unet_Dec_Blocks(1, \"Dec_Block1\", 512,  (3,3), En_Block5, En_Block4)\n","    Dec_Block2 = Unet_Dec_Blocks(2, \"Dec_Block2\", 256, (3,3), Dec_Block1, En_Block3)\n","    Dec_Block3 = Unet_Dec_Blocks(3, \"Dec_Block3\", 128, (3,3), Dec_Block2, En_Block2)\n","    Output_layer = Unet_Dec_Blocks(4, \"Dec_Block4\", 64, (3,3), Dec_Block3, En_Block1)\n"," \n","    Unet_model = Model(Input_layer, Output_layer)\n"," \n","    return Unet_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J2es2H1_yj3T"},"source":["### Deeplab_V3 Definition for prediction"]},{"cell_type":"code","metadata":{"id":"5exsk4PJL4Qj"},"source":["def Deeplab_V3(Input_shape, n_classes):\n","\n","    def Restnet_for_Deeplab(Input_layer):\n","            \n","        def Restnet_Conv_Block(Block_Number, Sub_block, Name, Filters, Previous_layer, initialize=\"he_normal\"):\n","            \n","            Multi_grid, Rate = [1,2,4], 2\n","            strides=(1,1) if Block_Number==2 or Block_Number==5 else (2,2)        \n","            \n","            if (Block_Number==5) and (Sub_block==1):\n","                D_Rate=Multi_grid[0] * Rate\n","            elif (Block_Number==5) and (Sub_block==2):\n","                D_Rate=Multi_grid[1] * Rate\n","            elif (Block_Number==5) and (Sub_block==3):\n","                D_Rate=Multi_grid[2] * Rate\n","            else:D_Rate=1\n","            \n","            Convolution1 = tf.keras.layers.Conv2D(Filters[0],(1,1), strides=strides, name= Name+\"Conv1\", activation = 'relu', kernel_initializer=initialize)(Previous_layer)\n","            Batch_norm1=tf.keras.layers.BatchNormalization()(Convolution1)\n","            \n","            Convolution2 = tf.keras.layers.Conv2D(Filters[1],(3,3), dilation_rate=D_Rate, name= Name+\"Conv2\", padding='same', activation = 'relu', kernel_initializer=initialize)(Batch_norm1)\n","            Batch_norm2=tf.keras.layers.BatchNormalization()(Convolution2)\n","            \n","            Convolution3 = tf.keras.layers.Conv2D(Filters[2],(1,1),name= Name+\"Conv3\", activation = None, kernel_initializer=initialize)(Batch_norm2)\n","            Batch_norm3=tf.keras.layers.BatchNormalization()(Convolution3)\n","            \n","            Layer_to_add = tf.keras.layers.Conv2D(Filters[2],(1,1), strides=strides, name= Name+\"conv_prep_add\", activation = None, kernel_initializer=initialize)(Previous_layer)\n","            \n","            Layer_to_add=tf.keras.layers.BatchNormalization()(Layer_to_add)\n","            \n","            Added_Layer=tf.keras.layers.add([Batch_norm3,Layer_to_add])\n","            Final_Conv=tf.keras.layers.Activation(\"relu\")(Added_Layer)\n","            \n","            return Final_Conv\n","            \n","        def Restnet_Id_Block(Block_Number,Sub_block,Name,Filters,Previous_layer,initialize=\"he_normal\"):\n","            \n","            Multi_grid, Rate = [1,2,4], 2\n","            if (Block_Number==5) and (Sub_block==1):\n","                D_Rate=Multi_grid[0] * Rate\n","            elif (Block_Number==5) and (Sub_block==2):\n","                D_Rate=Multi_grid[1] * Rate\n","            elif (Block_Number==5) and (Sub_block==3):\n","                D_Rate=Multi_grid[2] * Rate\n","            else:D_Rate=1\n","            \n","            Convolution1 = tf.keras.layers.Conv2D(Filters[0], (1,1), name= Name+\"Conv1\", activation = 'relu', kernel_initializer=initialize)(Previous_layer)\n","            Batch_norm1=tf.keras.layers.BatchNormalization()(Convolution1)\n","            \n","            Convolution2 = tf.keras.layers.Conv2D(Filters[1], (3,3), dilation_rate=D_Rate, name= Name+\"Conv2\", padding='same', activation = 'relu', kernel_initializer=initialize)(Batch_norm1)\n","            Batch_norm2=tf.keras.layers.BatchNormalization()(Convolution2)\n","            \n","            Convolution3 = tf.keras.layers.Conv2D(Filters[2], (1,1), name= Name+\"Conv3\", activation = None, kernel_initializer=initialize)(Batch_norm2)\n","            Batch_norm3=tf.keras.layers.BatchNormalization()(Convolution3)\n","            \n","            Added_Layer=tf.keras.layers.add([Batch_norm3,Previous_layer])\n","            Final_Conv1=tf.keras.layers.Activation(\"relu\")(Added_Layer)\n","    \n","            return Final_Conv1\n","        \n","        Block_1_Conv = tf.keras.layers.Conv2D(64,(7,7),strides=(2,2),name= \"Block1_Conv1\", activation = 'relu', padding='same', kernel_initializer=\"he_normal\")(Input_layer)\n","        Block_1_Batch_norm1=tf.keras.layers.BatchNormalization(name=\"Block1_Conv1_BN\")(Block_1_Conv)\n","        Block_1_MaxPool = tf.keras.layers.MaxPooling2D(pool_size=(3,3),strides=(2,2), name=\"Block1_Maxpool1\", padding='same')(Block_1_Batch_norm1)\n","            \n","        Block2_1_con= Restnet_Conv_Block(2,1,\"Block2.1_CONV_\", [64,64,256], Block_1_MaxPool)\n","        Block2_2_ID= Restnet_Id_Block( 2, 2,\"Block2.2_ID_\",    [64,64,256], Block2_1_con)\n","        Block2_3_ID= Restnet_Id_Block( 2, 3,\"Block2.3_ID_\",    [64,64,256], Block2_2_ID)\n","        \n","        Block3_1_con= Restnet_Conv_Block(3, 1,\"Block3.1_CONV_\", [128,128,512], Block2_3_ID)\n","        Block3_2_ID= Restnet_Id_Block( 3,   2, \"Block3.2_ID_\",   [128,128,512], Block3_1_con)\n","        Block3_3_ID= Restnet_Id_Block( 3,   3, \"Block3.3_ID_\",   [128,128,512], Block3_2_ID)\n","        Block3_4_ID= Restnet_Id_Block( 3,   4, \"Block3.4_ID_\",   [128,128,512], Block3_3_ID)\n","        \n","        Block4_1_con= Restnet_Conv_Block(4, 1, \"Block4.1_CONV_\", [256,256,1024], Block3_4_ID)\n","        Block4_2_ID= Restnet_Id_Block( 4,   2, \"Block4.2_ID_\",   [256,256,1024], Block4_1_con)\n","        Block4_3_ID= Restnet_Id_Block( 4,   3, \"Block4.3_ID_\",   [256,256,1024], Block4_2_ID)\n","        Block4_4_ID= Restnet_Id_Block( 4,   4, \"Block4.4_ID_\",   [256,256,1024], Block4_3_ID)\n","        Block4_5_ID= Restnet_Id_Block( 4,   5, \"Block4.5_ID_\",   [256,256,1024], Block4_4_ID)\n","        Block4_6_ID= Restnet_Id_Block( 4,   6, \"Block4.6_ID_\",   [256,256,1024], Block4_5_ID)\n","        \n","        Block5_1_con= Restnet_Conv_Block(5, 1, \"Block5.1_CONV_\", [512,512,2048], Block4_6_ID)\n","        Block5_2_ID= Restnet_Id_Block( 5,   2, \"Block5.2_ID_\",   [512,512,2048], Block5_1_con)\n","        Block5_3_ID= Restnet_Id_Block( 5,   3, \"Block5.3_ID_\",   [512,512,2048], Block5_2_ID)\n","        \n","        return Block5_3_ID\n","    \n","    def Atrous_Spatial_Pyramid_Pooling(Restnet_Block,ASPP_Filter=256):\n","\n","        G_pool = tf.keras.layers.GlobalAveragePooling2D(name='ASPP_GL_POOL')(Restnet_Block)\n","        G_pool = tf.keras.layers.Reshape((1,1,Restnet_Block.shape[3]))(G_pool)\n","        \n","        Convolution1 = tf.keras.layers.Conv2D(ASPP_Filter, (1,1), name= \"Global_Conv\", activation = \"relu\")(G_pool)\n","        Batch_norm1=tf.keras.layers.BatchNormalization()(Convolution1)\n","        \n","        Global_Features = UpSampling2D( size=(15,30), interpolation='bilinear', name='upsamp')(Batch_norm1)\n","        \n","        ASPP_Conv0 = tf.keras.layers.Conv2D(ASPP_Filter, (1,1), name= \"ASPP_Conv0_1x1\", activation = \"relu\", padding=\"same\")(Restnet_Block)\n","        ASPP_Conv0=tf.keras.layers.BatchNormalization()(ASPP_Conv0)\n","        \n","        ASPP_Conv1 = tf.keras.layers.Conv2D(ASPP_Filter, (3,3), name= \"ASPP_Conv1_3x3\", dilation_rate=6, activation = \"relu\", padding=\"same\")(Restnet_Block)\n","        ASPP_Conv1=tf.keras.layers.BatchNormalization()(ASPP_Conv1)\n","        \n","        ASPP_Conv2 = tf.keras.layers.Conv2D(ASPP_Filter, (3,3), name= \"ASPP_Conv2_3x3\", dilation_rate=12, activation = \"relu\", padding=\"same\")(Restnet_Block)\n","        ASPP_Conv2=tf.keras.layers.BatchNormalization()(ASPP_Conv2)\n","        \n","        ASPP_Conv3 = tf.keras.layers.Conv2D(ASPP_Filter, (3,3), name= \"ASPP_Conv3_3x3\", dilation_rate=18, activation = \"relu\", padding=\"same\")(Restnet_Block)\n","        ASPP_Conv3=tf.keras.layers.BatchNormalization()(ASPP_Conv3)\n","        \n","        ASPP_Features= tf.keras.layers.concatenate([Global_Features, ASPP_Conv0, ASPP_Conv1, ASPP_Conv2, ASPP_Conv3])\n","        ASPP_Features = tf.keras.layers.Conv2D(ASPP_Filter, (1,1), name= \"ASPP_Out\", activation = \"relu\")(ASPP_Features)\n","        ASPP_Features=tf.keras.layers.BatchNormalization()(ASPP_Features)\n","        \n","        ASPP_Features = UpSampling2D(size=(16,16), interpolation='bilinear', name='upsampling')(ASPP_Features)\n","        \n","        ASPP_Features = tf.keras.layers.Conv2D(64, (1,1), name= \"out\", activation = \"relu\")(ASPP_Features)\n","        \n","        ASPP_Features = tf.keras.layers.Conv2D(n_classes, (1,1), name= \"output\", activation ='relu')(ASPP_Features)\n","        \n","        Output=Activation('softmax', name=\"Softmax\")(ASPP_Features)\n","        \n","        return Output\n","    \n","    Input_layer = tf.keras.layers.Input(shape=Input_shape)\n","    Block5_3_ID= Restnet_for_Deeplab(Input_layer)\n","    Output=Atrous_Spatial_Pyramid_Pooling(Block5_3_ID)\n","    model = Model(inputs=Input_layer, outputs=Output, name=\"Deeplab_V3\")\n","    \n","    return model "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EU3js0s1yEep"},"source":["### Segnet Definition for prediction"]},{"cell_type":"code","metadata":{"id":"QxMyIfOWCSrB"},"source":["from keras.layers import *\n","from keras.models import *\n","import numpy as np\n","\n","def MaxUnpooling2D(pool, ind, batch_size, name):\n","    with tf.compat.v1.variable_scope(name):\n","        output_shape=[None,pool.shape[1]*2,pool.shape[2]*2,pool.shape[3]]\n","        pool_ = tf.reshape(pool, [-1])\n","        batch_range = tf.reshape(tf.range(batch_size, dtype=ind.dtype), [tf.shape(pool)[0], 1, 1, 1])\n","        b = tf.ones_like(ind) * batch_range\n","        b = tf.reshape(b, [-1, 1])\n","        ind_ = tf.reshape(ind, [-1, 1])\n","        ind_ = tf.concat([b, ind_], 1)\n","        ret = tf.scatter_nd(ind_, pool_, shape=[batch_size, output_shape[1] * output_shape[2] * output_shape[3]])\n","        ret = tf.reshape(ret, [tf.shape(pool)[0], output_shape[1], output_shape[2], output_shape[3]])\n","        return ret\n","\n","def Segnet_Segmentation(input_shape, n_labels, batch_size, Kernel=(3,3), Pool_Filter=(2,2), output_mode= \"softmax\"):\n","  \n","  Inputs, batch_size = Input(shape=input_shape), batch_size\n","  \n","  def Segnet_Encoder(Block_Number, Filters, Input_layer):\n","    \n","    Layer_1 = Convolution2D(Filters, Kernel, name= \"En_Block\"+str(Block_Number)+\"_Conv1\", activation = 'relu', padding= \"same\",  kernel_initializer= \"he_normal\")(Input_layer)\n","    Layer_1 = BatchNormalization(name= \"En_Block\"+str(Block_Number)+\"_Batch1\")(Layer_1)\n","\n","    Layer_2 = Convolution2D(Filters, Kernel, name= \"En_Block\"+str(Block_Number)+\"_Conv2\", activation = 'relu', padding= \"same\",  kernel_initializer= \"he_normal\")(Layer_1)\n","    Layer_2 = BatchNormalization(name= \"En_Block\"+str(Block_Number)+\"_Batch2\")(Layer_2)\n","        \n","    if Block_Number>2:\n","      Layer_3 = Convolution2D(Filters, Kernel, name= \"En_Block\"+str(Block_Number)+\"_Conv3\", activation = 'relu', padding= \"same\",  kernel_initializer= \"he_normal\")(Layer_2)\n","      Layer_3 = BatchNormalization(name= \"En_Block\"+str(Block_Number)+\"_Batch3\")(Layer_3)\n","      \n","      if Block_Number==5:\n","        Layer_3=ZeroPadding2D(((1,0),(0,0)), name='Zero_pad')(Layer_3)\n","      return Layer_3\n","    \n","    return Layer_2\n","\n","  def Segnet_Decoder(Block_Number, Filters, Input_layer):\n","\n","    Get_filter = lambda x : int(Filters/2) if (((Block_Number > 2 and Block_Number < 5) and (x==3)) or (Block_Number==2 and x==2)) else Filters\n","    \n","    if Block_Number==5:\n","        Input_layer=Cropping2D(((1, 0),(0,0)))(Input_layer)\n","    \n","    Layer_1 = Convolution2D(Get_filter(1), Kernel, name= \"Dec_Block\"+str(Block_Number)+\"_Conv1\", activation = 'relu', padding= \"same\",  kernel_initializer= \"he_normal\")(Input_layer)\n","    Layer_1 = BatchNormalization(name= \"Dec_Block\"+str(Block_Number)+\"_Batch1\")(Layer_1)\n","\n","    Layer_2 = Convolution2D(Get_filter(2), Kernel, name= \"Dec_Block\"+str(Block_Number)+\"_Conv2\", activation = 'relu', padding= \"same\",  kernel_initializer= \"he_normal\")(Layer_1)\n","    Layer_2 = BatchNormalization(name= \"Dec_Block\"+str(Block_Number)+\"_Batch2\")(Layer_2)\n","    \n","    if Block_Number>2:\n","      Layer_3 = Convolution2D(Get_filter(3), Kernel, name= \"Dec_Block\"+str(Block_Number)+\"_Conv3\", activation = 'relu', padding= \"same\",  kernel_initializer= \"he_normal\")(Layer_2)\n","      Layer_3 = BatchNormalization(name= \"Dec_Block\"+str(Block_Number)+\"_Batch3\")(Layer_3)\n","      return Layer_3\n","    return Layer_2\n","\n","  Encoder_Conv1=Segnet_Encoder(1, 64, Inputs)\n","  max_pool1,pool_indices_1 =tf.nn.max_pool_with_argmax(Encoder_Conv1, [1, 2, 2, 1], [1, 2, 2, 1], padding=\"VALID\")\n","\n","  Encoder_Conv2=Segnet_Encoder(2, 128, max_pool1)\n","  max_pool_2,pool_indices_2 =tf.nn.max_pool_with_argmax(Encoder_Conv2, [1, 2, 2, 1], [1, 2, 2, 1], padding=\"VALID\")\n","\n","  Encoder_Conv3=Segnet_Encoder(3, 256, max_pool_2)    \n","  max_pool_3,pool_indices_3 =tf.nn.max_pool_with_argmax(Encoder_Conv3, [1, 2, 2, 1], [1, 2, 2, 1], padding=\"VALID\")\n","\n","  Encoder_Conv4=Segnet_Encoder(4, 512, max_pool_3)\n","  max_pool_4,pool_indices_4 =tf.nn.max_pool_with_argmax(Encoder_Conv4, [1, 2, 2, 1], [1, 2, 2, 1], padding=\"VALID\")\n","\n","  Encoder_Conv5=Segnet_Encoder(5, 512, max_pool_4)    \n","  max_pool_5,pool_indices_5 =tf.nn.max_pool_with_argmax(Encoder_Conv5, [1, 2, 2, 1], [1, 2, 2, 1], padding=\"VALID\")\n","\n","  Decoder_upsamp5 =MaxUnpooling2D(max_pool_5, pool_indices_5,  batch_size, name=\"un_pool_5\")\n","  Decoder_Conv5=Segnet_Decoder(5, 512, Decoder_upsamp5)\n","\n","  Decoder_upsamp4 =MaxUnpooling2D(Decoder_Conv5, pool_indices_4, batch_size, name=\"un_pool_4\")  \n","  Decoder_Conv4=Segnet_Decoder(4, 512, Decoder_upsamp4)\n","\n","  Decoder_upsamp3 =MaxUnpooling2D(Decoder_Conv4, pool_indices_3, batch_size, name=\"un_pool_3\") \n","  Decoder_Conv3=Segnet_Decoder(3, 256, Decoder_upsamp3)\n","\n","  Decoder_upsamp2 =MaxUnpooling2D(Decoder_Conv3, pool_indices_2, batch_size, name=\"un_pool_2\")   \n","  Decoder_Conv2=Segnet_Decoder(2, 128, Decoder_upsamp2)\n","\n","  Decoder_upsamp1 =MaxUnpooling2D(Decoder_Conv2, pool_indices_1, batch_size, name=\"un_pool_1\")   \n","  Decoder_Conv1=Segnet_Decoder(1, 64, Decoder_upsamp1)\n","  \n","  Convolution_out = tf.keras.layers.Conv2D(n_labels, (3,3), name= \"Final_Conv\", activation = 'relu', kernel_initializer=\"he_normal\", padding ='same')(Decoder_Conv1)\n","  Output=Activation('softmax', name=\"Softmax\")(Convolution_out)\n","  \n","  model = Model(inputs=Inputs, outputs=Output, name=\"SEGNET\")\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dtRCkVjhytkw"},"source":["### PSPNET Definition for prediction"]},{"cell_type":"code","metadata":{"id":"MNq_hBjp9v5W"},"source":["def PSPNET(Input_shape, n_classes):    \n","    \n","    def Restnet50_Module(Input_layer, n_classes):\n","        \n","        def Restnet_Conv_Block(Block_Number,Name,Filters,Previous_layer,initialize=\"he_normal\"):\n","            \n","            strides=(1,1) if Block_Number==2 else (2,2)\n","            \n","            if (Block_Number==4):\n","                D_Rate=2\n","            elif (Block_Number==5):\n","                D_Rate=4\n","            else:\n","                D_Rate=1\n","            \n","            Convolution1 = tf.keras.layers.Conv2D(Filters[0],(1,1), strides=strides, name= Name+\"Conv1\", activation = 'relu', kernel_initializer=initialize)(Previous_layer)\n","            Batch_norm1=tf.keras.layers.BatchNormalization()(Convolution1)\n","        \n","            Convolution2 = tf.keras.layers.Conv2D(Filters[1],(3,3), dilation_rate=D_Rate, name= Name+\"Conv2\", padding='same', activation = 'relu', kernel_initializer=initialize)(Batch_norm1)\n","            Batch_norm2=tf.keras.layers.BatchNormalization()(Convolution2)\n","                \n","            Convolution3 = tf.keras.layers.Conv2D(Filters[2],(1,1),name= Name+\"Conv3\", activation = None, kernel_initializer=initialize)(Batch_norm2)\n","            Batch_norm3=tf.keras.layers.BatchNormalization()(Convolution3)\n","        \n","            Layer_to_add = tf.keras.layers.Conv2D(Filters[2],(1,1), strides=strides, name= Name+\"conv_prep_add\", activation = None, kernel_initializer=initialize)(Previous_layer)\n","            \n","            Layer_to_add=tf.keras.layers.BatchNormalization()(Layer_to_add)\n","            \n","            Added_Layer=tf.keras.layers.add([Batch_norm3,Layer_to_add])\n","            Final_Conv=tf.keras.layers.Activation(\"relu\")(Added_Layer)\n","            \n","            return Final_Conv\n","            \n","        def Restnet_Id_Block(Block_Number,Name,Filters,Previous_layer,initialize=\"he_normal\"):\n","            \n","            if (Block_Number==4):\n","                D_Rate=2\n","            elif (Block_Number==5):\n","                D_Rate=4\n","            else:\n","                D_Rate=1\n","                \n","            Convolution1 = tf.keras.layers.Conv2D(Filters[0], (1,1), name= Name+\"Conv1\", activation = 'relu', kernel_initializer=initialize)(Previous_layer)\n","            Batch_norm1=tf.keras.layers.BatchNormalization()(Convolution1)\n","            \n","            Convolution2 = tf.keras.layers.Conv2D(Filters[1], (3,3), dilation_rate=D_Rate, name= Name+\"Conv2\", padding='same', activation = 'relu', kernel_initializer=initialize)(Batch_norm1)\n","            Batch_norm2=tf.keras.layers.BatchNormalization()(Convolution2)\n","            \n","            Convolution3 = tf.keras.layers.Conv2D(Filters[2], (1,1), name= Name+\"Conv3\", activation = None, kernel_initializer=initialize)(Batch_norm2)\n","            Batch_norm3=tf.keras.layers.BatchNormalization()(Convolution3)\n","    \n","            Added_Layer=tf.keras.layers.add([Batch_norm3,Previous_layer])\n","            Final_Conv1=tf.keras.layers.Activation(\"relu\")(Added_Layer)\n","\n","            return Final_Conv1\n","    \n","        Block_1_Conv = tf.keras.layers.Conv2D(64,(7,7),strides=(2,2),name= \"Block1_Conv1\", activation = 'relu', kernel_initializer=\"he_normal\")(Input_layer)\n","        Block_1_Batch_norm1=tf.keras.layers.BatchNormalization(name=\"Block1_Conv1_BN\")(Block_1_Conv)\n","        Block_1_MaxPool = tf.keras.layers.MaxPooling2D(pool_size=(3,3),strides=(2,2), name=\"Block1_Maxpool1\")(Block_1_Batch_norm1)\n","\n","        Block2_1_con= Restnet_Conv_Block(2, \"Block2.1_CONV_\", [16,16,64], Block_1_MaxPool)\n","        Block2_2_ID= Restnet_Id_Block( 2,  \"Block2.2_ID_\",    [16,16,64], Block2_1_con)\n","        Block2_3_ID= Restnet_Id_Block( 2,  \"Block2.3_ID_\",    [16,16,64], Block2_2_ID)\n","\n","\n","        Block3_1_con= Restnet_Conv_Block(3, \"Block3.1_CONV_\", [32,32,128], Block2_3_ID)\n","        Block3_2_ID= Restnet_Id_Block( 3,   \"Block3.2_ID_\",   [32,32,128], Block3_1_con)\n","        Block3_3_ID= Restnet_Id_Block( 3,   \"Block3.3_ID_\",   [32,32,128], Block3_2_ID)\n","        Block3_4_ID= Restnet_Id_Block( 3,   \"Block3.4_ID_\",   [32,32,128], Block3_3_ID)\n","\n","        Block4_1_con= Restnet_Conv_Block(4, \"Block4.1_CONV_\", [64,64,256], Block3_4_ID)\n","        Block4_2_ID= Restnet_Id_Block( 4,   \"Block4.2_ID_\",   [64,64,256], Block4_1_con)\n","        Block4_3_ID= Restnet_Id_Block( 4,   \"Block4.3_ID_\",   [64,64,256], Block4_2_ID)\n","        Block4_4_ID= Restnet_Id_Block( 4,   \"Block4.4_ID_\",   [64,64,256], Block4_3_ID)\n","        Block4_5_ID= Restnet_Id_Block( 4,   \"Block4.5_ID_\",   [64,64,256], Block4_4_ID)\n","        Block4_6_ID= Restnet_Id_Block( 4,   \"Block4.6_ID_\",   [64,64,256], Block4_5_ID)\n","        \n","        Block5_1_con= Restnet_Conv_Block(5, \"Block5.1_CONV_\", [128,128,512], Block4_6_ID)\n","        Block5_2_ID= Restnet_Id_Block( 5,   \"Block5.2_ID_\",   [128,128,512], Block5_1_con)\n","        Block5_3_ID= Restnet_Id_Block( 5,   \"Block5.3_ID_\",   [128,128,512], Block5_2_ID)\n","        \n","        return Block5_3_ID\n","    \n","    def Pyramid_Module(Rest50_Layer):\n","        \n","        def Feature_Sub_Map(Sub_block, Pool_size, Previous_layer, filters=128):\n","            \n","            if Sub_block==\"RED\":\n","                Pool_size= (Previous_layer.shape[1],Previous_layer.shape[2])\n","                Sub_pool = tf.keras.layers.GlobalAveragePooling2D(name= Sub_block+'_GL_POOL')(Previous_layer)\n","                Sub_pool = tf.keras.layers.Reshape((1,1,Previous_layer.shape[3]))(Sub_pool)\n","            else:\n","                Sub_pool = tf.keras.layers.AveragePooling2D(pool_size=Pool_size,name= Sub_block+'_AVG_POOL')(Previous_layer)\n","            \n","            Conv_sub = tf.keras.layers.Convolution2D(filters=filters,kernel_size=(1,1),name= Sub_block+'Conv1_1')(Sub_pool)\n","            Conv_sub=tf.keras.layers.BatchNormalization()(Conv_sub)\n","            Retn_Sub = tf.keras.layers.UpSampling2D(size=Pool_size, name=Sub_block+'Up_sample',interpolation='bilinear')(Conv_sub)\n","            \n","            return Retn_Sub\n","        \n","        Rest50_Layer = UpSampling2D(size=(4,4),interpolation='bilinear',name='Size_Adjust_samp')(Rest50_Layer)\n","        Rest50_Layer=tf.keras.layers.ZeroPadding2D((2,0),name='up_Size_Adjust_pad')(Rest50_Layer)\n","        \n","        Red_Map= Feature_Sub_Map(    \"RED\",   (1,1), Rest50_Layer)\n","        Orange_Map= Feature_Sub_Map(\"ORANGE\", (2,2), Rest50_Layer)\n","        Blue_Map= Feature_Sub_Map(   \"BLUE\",  (3,3), Rest50_Layer)\n","        Green_Map= Feature_Sub_Map(  \"GREEN\", (6,6), Rest50_Layer)\n","    \n","        Global_Concat= tf.keras.layers.concatenate([Rest50_Layer, Green_Map, Blue_Map, Orange_Map, Red_Map])\n","        UpSampling = tf.keras.layers.UpSampling2D(size=(6,8),interpolation='bilinear',name='en_Size_Adjust_samp')(Global_Concat)\n","        \n","        Convolution1 = tf.keras.layers.Conv2D(64, (5,5),name= \"Conv1\", activation = 'relu', padding=\"same\",kernel_initializer=\"he_normal\")(ZeroPadding2D((12,0))(UpSampling))\n","        Convolution1=tf.keras.layers.BatchNormalization()(Convolution1)\n","    \n","        Convolution2 = tf.keras.layers.Conv2D(7, (3,3),name= \"Conv2\",activation = 'relu', padding=\"same\",kernel_initializer=\"he_normal\")(Convolution1)\n","        Output=Activation('softmax', name=\"Softmax\")(Convolution2)\n","        \n","        return Output\n","    \n","    Input_layer = tf.keras.layers.Input(shape=Input_shape)\n","    Rest50_Layer= Restnet50_Module(Input_layer, n_classes)\n","    Output=Pyramid_Module(Rest50_Layer)\n","    PSPNET_Model = Model(Input_layer, Output)\n","    \n","    return PSPNET_Model"],"execution_count":null,"outputs":[]}]}